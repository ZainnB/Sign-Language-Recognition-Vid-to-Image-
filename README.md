# Pakistan Sign Language (PSL) Landmark Recognition

This project implements a lightweight, efficient approach to recognize **Pakistan Sign Language (PSL)** signs using **MediaPipe Hand Landmarks** and **CNN-based image classification**, based on the research idea to convert videos into static spatial trajectory images.

---

## ğŸ§  Core Idea

Traditional models (like I3D, C3D, TSM) are powerful but **resource-heavy**, requiring GPUs and large video datasets.

Our method:
- ğŸ¥ Converts gesture **videos into single image trajectories** using MediaPipe.
- ğŸ–¼ï¸ Extracts **21 hand landmarks per frame** â†’ builds 21 binary images representing motion trajectory.
- ğŸ§  Trains a **CNN** on these images instead of video sequences â†’ significantly faster & deployable.

---

## ğŸ“ Directory Structure

```text
PSL_Landmark_Recognition
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ alphabets              # Original full gesture videos (A.mp4, B.mp4, ...)
â”‚   â”œâ”€â”€ vid1                   # First half (trimmed) of each letter video
â”‚   â”œâ”€â”€ vid1_old               # First half (no trimming) â€“ previously used
â”‚   â”œâ”€â”€ vid2                   # Second half (trimmed)
â”‚   â”œâ”€â”€ vid2_old               # Second half (no trimming)
â”‚   â”œâ”€â”€ landmark_images        # PNG images for each landmark trajectory (trimmed)
â”‚   â”‚   â”œâ”€â”€ WRIST
â”‚   â”‚   â”œâ”€â”€ INDEX_FINGER_TIP
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ landmark_images_old    # Same as above, but from untrimmed videos
â”‚   â””â”€â”€ aggregated             # Composite images of top-5 landmarks
â”‚       â”œâ”€â”€ train
â”‚       â””â”€â”€ test
â”œâ”€â”€ code
â”‚   â”œâ”€â”€ cnn_model.py           # CNN training on images
â”‚   â”œâ”€â”€ combine_top5.py        # Combines top-5 landmark images into one
â”‚   â”œâ”€â”€ split.py               # Splits each A.mp4 into A_1 and A_2
â”‚   â””â”€â”€ vid_to_img.py          # Converts video frames to 21 landmark images
â”œâ”€â”€ my_model.keras             # Saved trained model
â”œâ”€â”€ structure.txt              # Directory structure saved as text
â””â”€â”€ top5_landmarks.txt         # Landmarks with highest accuracy
```
---

## âš™ï¸ How It Works

### 1. **Preprocessing**
- All `A.mp4 â†’ Z.mp4` videos are split into two using `split.py`.
- First & second attempts per sign are stored in `vid1/` and `vid2/`.

### 2. **Optional Trimming**
To improve sign focus:
- We experimented with trimming **20% from start and end**.
- Results suggest trimmed versions improve focus on the actual sign.
- Non-trimmed (`*_old/`) videos may show higher accuracy but possibly learned irrelevant hand-raising/lowering patterns.

### 3. **Landmark Extraction**
- `vid_to_img.py` uses MediaPipe to extract 21 landmark trajectories.
- Each landmark becomes one static 128x128 image per video.

### 4. **Model Training**
- `cnn_model.py` trains a CNN per landmark.
- `top5_landmarks.txt` logs the best performing landmarks.
- `combine_top5.py` aggregates top-5 landmark images into composite samples for better accuracy (~77%).

---

## ğŸ§ª Observations & Learnings

- **Trimming video ends** helps focus on actual sign, avoiding noisy background motion.
- **Old dataset** (no trimming) showed better accuracy, but likely learned repetitive patterns (e.g., hand raise/drop), not the signs.
- **Aggregation of top landmarks** boosts accuracy by combining richer sign-specific features.

---

## ğŸš€ Future Directions

1. ğŸ” **Frame-level Cropping**:
   - Identify and retain only frames where sign is actively performed.
   - Discard raising/lowering frames for purer signal learning.

2. ğŸ“ˆ **Dataset Expansion**:
   - Add full words from PSL dictionary.
   - Retain same landmark and preprocessing pipeline for consistency.

3. ğŸ“± **Mobile Deployment**:
   - Convert CNN model to TFLite for edge deployment on phones or tablets.

---

## âœ… Current Accuracy (Trimmed + Aggregated)
- CNN Accuracy: **~77%**
- Based on top-5 landmark combination from 26 signs (Aâ€“Z), 2 samples each.

---

## âœï¸ Credits & Team Note

This work is part of our ongoing Final Year Project (FYP). The current repo reflects:
- A complete working pipeline
- Preprocessing logic & experiments
- Model performance insights

Special focus is on reproducibility, lightweight deployment, and improved understanding of sign language via efficient image-based modeling.

---
